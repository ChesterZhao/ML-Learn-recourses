{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:30:33.910422: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-12-13 22:30:33.910445: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-13 22:30:33.910451: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-13 22:30:33.910483: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-13 22:30:33.910496: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/zhaochengxin/anaconda3/envs/Learn/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/var/folders/fm/_6wfn0r96gjcpy7779d0j2fc0000gn/T/ipykernel_84314/242287152.py\", line 34, in categorical_Entropy_Loss_Function  *\n        return -tf.reduce_mean(y_true * tf.math.log(y_pred))\n\n    ValueError: Dimensions must be equal, but are 32 and 10 for '{{node categorical_Entropy_Loss_Function/mul}} = Mul[T=DT_FLOAT](IteratorGetNext:1, categorical_Entropy_Loss_Function/Log)' with input shapes: [?,32], [?,10].\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 141\u001B[0m\n\u001B[1;32m    139\u001B[0m ann \u001B[38;5;241m=\u001B[39m ANNClassifier(input_shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m3\u001B[39m), num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, hidden_layers_sizes\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m])\n\u001B[1;32m    140\u001B[0m ann\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 141\u001B[0m \u001B[43mann\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;66;03m# SHAP plot\u001B[39;00m\n\u001B[1;32m    144\u001B[0m ann\u001B[38;5;241m.\u001B[39mSHAP_plot(X_test, y_test, \u001B[38;5;241m0\u001B[39m)\n",
      "Cell \u001B[0;32mIn[1], line 78\u001B[0m, in \u001B[0;36mANNClassifier.fit\u001B[0;34m(self, X, y, epochs, batch_size, validation_data)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, epochs, batch_size, validation_data):\n\u001B[1;32m     69\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    Fits the model to the training data.\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    :param X: the training data\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;124;03m    :return: the history of the training process\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 78\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhistory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhistory\n",
      "File \u001B[0;32m~/anaconda3/envs/Learn/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/fm/_6wfn0r96gjcpy7779d0j2fc0000gn/T/__autograph_generated_filekyix8x3g.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/var/folders/fm/_6wfn0r96gjcpy7779d0j2fc0000gn/T/__autograph_generated_file1m8nun9j.py:12\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__categorical_Entropy_Loss_Function\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mreduce_mean, ((ag__\u001B[38;5;241m.\u001B[39mld(y_true) \u001B[38;5;241m*\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mmath\u001B[38;5;241m.\u001B[39mlog, (ag__\u001B[38;5;241m.\u001B[39mld(y_pred),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope))\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/Users/zhaochengxin/anaconda3/envs/Learn/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/var/folders/fm/_6wfn0r96gjcpy7779d0j2fc0000gn/T/ipykernel_84314/242287152.py\", line 34, in categorical_Entropy_Loss_Function  *\n        return -tf.reduce_mean(y_true * tf.math.log(y_pred))\n\n    ValueError: Dimensions must be equal, but are 32 and 10 for '{{node categorical_Entropy_Loss_Function/mul}} = Mul[T=DT_FLOAT](IteratorGetNext:1, categorical_Entropy_Loss_Function/Log)' with input shapes: [?,32], [?,10].\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Custom activation functions\n",
    "def relu(x):\n",
    "    return tf.maximum(0.0, x)\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * tf.sigmoid(x)\n",
    "\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return tf.where(x >= 0.0, x, alpha * (tf.exp(x) - 1))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return tf.exp(x) / tf.reduce_sum(tf.exp(x))\n",
    "\n",
    "\n",
    "# Custom loss function\n",
    "def categorical_Entropy_Loss_Function(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_true * tf.math.log(y_pred))\n",
    "\n",
    "\n",
    "# Custom accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_predictions = tf.equal(tf.argmax(y_true, axis=1), tf.argmax(y_pred, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class ANNClassifier:\n",
    "    def __init__(self, input_shape, num_classes, hidden_layers_sizes):\n",
    "        \"\"\"\n",
    "        The constructor of the ANNClassifier class.\n",
    "        :param input_shape: The shape of the input data\n",
    "        :param num_classes: The number of classes\n",
    "        :param hidden_layers_sizes: The sizes of the hidden layers\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Flatten(input_shape=input_shape))\n",
    "        for i, layer_size in enumerate(hidden_layers_sizes):\n",
    "            if i % 2 == 0:\n",
    "                self.model.add(Dense(layer_size, activation=swish))\n",
    "            else:\n",
    "                self.model.add(Dense(layer_size, activation='elu'))\n",
    "        self.model.add(Dense(num_classes, activation=softmax))\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        \"\"\"\n",
    "        Compiles the model.\n",
    "        :param optimizer: The optimizer to use\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=optimizer, loss=categorical_Entropy_Loss_Function, metrics=[accuracy])\n",
    "\n",
    "    def fit(self, X, y, epochs, batch_size, validation_data):\n",
    "        \"\"\"\n",
    "        Fits the model to the training data.\n",
    "        :param X: the training data\n",
    "        :param y: the training labels\n",
    "        :param epochs: the number of epochs\n",
    "        :param batch_size: the batch size\n",
    "        :param validation_data: the validation data and labels\n",
    "        :return: the history of the training process\n",
    "        \"\"\"\n",
    "        self.history = self.model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "        return self.history\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Explains the model's predictions.\n",
    "        :param X: the data used for prediction\n",
    "        :return: the explanation\n",
    "        \"\"\"\n",
    "        explainer = shap.DeepExplainer(self.model, X)\n",
    "        return explainer.shap_values(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the labels of the data.\n",
    "        :param X: the data used for prediction\n",
    "        :return: the predicted labels\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    \n",
    "    def SHAP_plot(self, X, y, index):\n",
    "        \"\"\"\n",
    "        Plots the SHAP values.\n",
    "        :param X: the data used for prediction\n",
    "        :param y: the labels\n",
    "        :param index: the index of the image to explain\n",
    "        \"\"\"\n",
    "        explainer = shap.DeepExplainer(self.model, X)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        shap.image_plot(shap_values[index], X[index], show=False)\n",
    "        plt.title('Predicted: ' + str(np.argmax(self.predict(X)[index])) + ' Actual: ' + str(np.argmax(y[index])))\n",
    "        plt.show()\n",
    "   \n",
    "\n",
    "# Function to load images\n",
    "def load_images(image_paths, target_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image_path in image_paths:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = img.resize(target_size)\n",
    "        img_array = np.array(img)\n",
    "        images.append(img_array)\n",
    "        label = os.path.basename(os.path.dirname(image_path))\n",
    "        labels.append(label)\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "# Load and process images\n",
    "image_paths = glob.glob('Car_Logo_Dataset/**/*.png', recursive=True)\n",
    "X, y = load_images(image_paths, target_size=(32, 32))\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ann = ANNClassifier(input_shape=(32, 32, 3), num_classes=10, hidden_layers_sizes=[128, 128, 128, 128, 128, 128, 128, 128, 128, 128])\n",
    "ann.compile(optimizer='adam')\n",
    "ann.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# SHAP plot\n",
    "ann.SHAP_plot(X_test, y_test, 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:30:34.920537Z",
     "start_time": "2023-12-13T14:30:27.628111Z"
    }
   },
   "id": "bd7b34a93b2465c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-13T14:30:34.918508Z"
    }
   },
   "id": "61700574756801b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
