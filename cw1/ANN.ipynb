{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e566f44fb9d65ee"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b75bb4de9908ad7",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.438374Z",
     "start_time": "2023-11-21T10:39:22.286973Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efc2330d325cef25"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    \"erythema\", \"scaling\", \"definite borders\", \"itching\", \"koebner phenomenon\",\n",
    "    \"polygonal papules\", \"follicular papules\", \"oral mucosal involvement\", \"knee and elbow involvement\",\n",
    "    \"scalp involvement\", \"family history\", \"melanin incontinence\", \"eosinophils in the infiltrate\",\n",
    "    \"PNL infiltrate\", \"fibrosis of the papillary dermis\", \"exocytosis\", \"acanthosis\",\n",
    "    \"hyperkeratosis\", \"parakeratosis\", \"clubbing of the rete ridges\", \"elongation of the rete ridges\",\n",
    "    \"thinning of the suprapapillary epidermis\", \"spongiform pustule\", \"munro microabcess\",\n",
    "    \"focal hypergranulosis\", \"disappearance of the granular layer\", \"vacuolisation and damage of basal layer\",\n",
    "    \"spongiosis\", \"saw-tooth appearance of retes\", \"follicular horn plug\", \"perifollicular parakeratosis\",\n",
    "    \"inflammatory monoluclear inflitrate\", \"band-like infiltrate\", \"Age\", \"class\"\n",
    "]\n",
    "\n",
    "dermatology_data = pd.read_csv('dermatology/dermatology.data', header=None, names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.444730Z",
     "start_time": "2023-11-21T10:39:22.439388Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Data Preprocessing\n",
    "## 3.1 Data replace and fill missing values\n",
    "- Replace the missing values with NaN\n",
    "- Convert the 'Age' column to numeric\n",
    "- Fill the missing values in 'Age' with the median age\n",
    "- Drop the 'perifollicular parakeratosis' column(because all values are 0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa9f28416dd28e2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Replacing the missing values with NaN\n",
    "dermatology_data.replace(\"?\", pd.NA, inplace=True)\n",
    "dermatology_data['Age'] = pd.to_numeric(dermatology_data['Age'], errors='coerce')\n",
    "\n",
    "# Filling the missing values in 'Age' with the median age\n",
    "age_median = dermatology_data['Age'].median()\n",
    "dermatology_data['Age'].fillna(age_median, inplace=True)\n",
    "\n",
    "# Dropping the 'perifollicular parakeratosis' column\n",
    "dermatology_data.drop('perifollicular parakeratosis', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.449960Z",
     "start_time": "2023-11-21T10:39:22.446444Z"
    }
   },
   "id": "e4ca06310ab7db2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Data Scaling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12c008b690be4541"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   erythema   scaling  definite borders   itching  koebner phenomenon  \\\n0  0.666667  0.666667          0.000000  1.000000            0.000000   \n1  1.000000  1.000000          1.000000  0.666667            0.333333   \n2  0.666667  0.333333          0.666667  1.000000            0.333333   \n3  0.666667  0.666667          0.666667  0.000000            0.000000   \n4  0.666667  1.000000          0.666667  0.666667            0.666667   \n\n   polygonal papules  follicular papules  oral mucosal involvement  \\\n0           0.000000                 0.0                  0.000000   \n1           0.000000                 0.0                  0.000000   \n2           1.000000                 0.0                  1.000000   \n3           0.000000                 0.0                  0.000000   \n4           0.666667                 0.0                  0.666667   \n\n   knee and elbow involvement  scalp involvement  ...  focal hypergranulosis  \\\n0                    0.333333           0.000000  ...               0.000000   \n1                    0.333333           0.333333  ...               0.000000   \n2                    0.000000           0.000000  ...               0.666667   \n3                    1.000000           0.666667  ...               0.000000   \n4                    0.000000           0.000000  ...               0.666667   \n\n   disappearance of the granular layer  \\\n0                             0.000000   \n1                             0.000000   \n2                             0.000000   \n3                             1.000000   \n4                             0.666667   \n\n   vacuolisation and damage of basal layer  spongiosis  \\\n0                                 0.000000    1.000000   \n1                                 0.000000    0.000000   \n2                                 0.666667    1.000000   \n3                                 0.000000    0.000000   \n4                                 1.000000    0.666667   \n\n   saw-tooth appearance of retes  follicular horn plug  \\\n0                       0.000000                   0.0   \n1                       0.000000                   0.0   \n2                       0.666667                   0.0   \n3                       0.000000                   0.0   \n4                       1.000000                   0.0   \n\n   inflammatory monoluclear inflitrate  band-like infiltrate       Age  class  \n0                             0.333333                   0.0  0.733333      2  \n1                             0.333333                   0.0  0.106667      1  \n2                             0.666667                   1.0  0.346667      3  \n3                             1.000000                   0.0  0.533333      1  \n4                             0.666667                   1.0  0.600000      3  \n\n[5 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>erythema</th>\n      <th>scaling</th>\n      <th>definite borders</th>\n      <th>itching</th>\n      <th>koebner phenomenon</th>\n      <th>polygonal papules</th>\n      <th>follicular papules</th>\n      <th>oral mucosal involvement</th>\n      <th>knee and elbow involvement</th>\n      <th>scalp involvement</th>\n      <th>...</th>\n      <th>focal hypergranulosis</th>\n      <th>disappearance of the granular layer</th>\n      <th>vacuolisation and damage of basal layer</th>\n      <th>spongiosis</th>\n      <th>saw-tooth appearance of retes</th>\n      <th>follicular horn plug</th>\n      <th>inflammatory monoluclear inflitrate</th>\n      <th>band-like infiltrate</th>\n      <th>Age</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>0.0</td>\n      <td>0.733333</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>0.0</td>\n      <td>0.106667</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>1.0</td>\n      <td>0.346667</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.533333</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>1.0</td>\n      <td>0.600000</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the data\n",
    "dermatology_data_scaled = dermatology_data.copy()\n",
    "for column in dermatology_data.columns[:-1]:\n",
    "    col_min = dermatology_data[column].min()\n",
    "    col_max = dermatology_data[column].max()\n",
    "    dermatology_data_scaled[column] = (dermatology_data[column] - col_min) / (col_max - col_min)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.472447Z",
     "start_time": "2023-11-21T10:39:22.451927Z"
    }
   },
   "id": "c4c40237672db3c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Data Splitting\n",
    "- Split the data into features and target\n",
    "- One-hot encode the target, to get 6 output nodes\n",
    "- Split the data into train and test sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e64f64c483e5eb03"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaochengxin/anaconda3/envs/Learn/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into features and target\n",
    "X = dermatology_data_scaled.drop('class', axis=1).values\n",
    "y = dermatology_data_scaled['class'].values\n",
    "\n",
    "# One-hot encoding the target, to get 6 output nodes, it will more fit to train ANN model\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.473186Z",
     "start_time": "2023-11-21T10:39:22.467796Z"
    }
   },
   "id": "69c284527853f441"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. ANN Model\n",
    "## 4.1 Tools preparation\n",
    "- I choose use sigmoid as my activation function\n",
    "- I choose use cross-entropy as my loss function\n",
    "- Due to the output is multi-class, I choose use softmax as my activation function for output layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec15becfac5cfedf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    To compute the sigmoid function for the input x\n",
    "    :param x: the input data\n",
    "    :return: the sigmoid function for the input x\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    To compute the derivative of the sigmoid function for the input x\n",
    "    :param x: the input data\n",
    "    :return: the derivative of the sigmoid function for the input x\n",
    "    \"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    To compute the softmax function for the input x, due to the output is multi-class, I choose use softmax as my activation function for output layer\n",
    "    :param x: the input data\n",
    "    :return: the softmax function for the input x\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    To compute the cross-entropy loss for the input y_true and y_pred, due to the output is multi-class, I choose use cross-entropy as my loss function\n",
    "    :param y_true: the true labels\n",
    "    :param y_pred: the predicted labels\n",
    "    :return: the cross-entropy loss for the input y_true and y_pred\n",
    "    \"\"\"\n",
    "    return -np.sum(y_true * np.log(y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.480673Z",
     "start_time": "2023-11-21T10:39:22.474401Z"
    }
   },
   "id": "851eded2265baecf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 ANN implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd4c1573625c20d8"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        - I choose use 2 layers ANN model, the first layer is hidden layer, the second layer is output layer. I used the random \"randn\" function to initialize the weights to get more accuracy and help the model to fitting the data better.\n",
    "        - For the bias, I used the zeros function to initialize the bias.\n",
    "        :param input_size: the number of input nodes\n",
    "        :param hidden_size: the number of hidden nodes\n",
    "        :param output_size: the number of output nodes\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.weights1 = np.random.randn(input_size, hidden_size)\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.weights2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.cost_history = []\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        \"\"\"\n",
    "        In the feedforward pass, the weighted sum of the inputs and the bias is computed for each node in the hidden layer. The result is then passed through the sigmoid activation function to get the output of the hidden layer. The same process is repeated for the output layer, but the activation function used is softmax.\n",
    "        :param X: the input data - X\n",
    "        :return: the output of the output layer\n",
    "        \"\"\"\n",
    "        self.layer1 = sigmoid(np.dot(X, self.weights1) + self.bias1)\n",
    "        self.output = softmax(np.dot(self.layer1, self.weights2) + self.bias2)\n",
    "        return self.output\n",
    "\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        \"\"\"\n",
    "        In the backpropagation pass, the error is computed for the output layer. The weights and biases are then updated using the error and the learning rate. The same process is repeated for the hidden layer.\n",
    "        :param X: the input data - X\n",
    "        :param y: the target data - y\n",
    "        :param learning_rate: the learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Calculate the error and delta for each layer, due to the output is multi-class, I choose use cross-entropy as my loss function and the softmax as my activation function for output layer, so the output_delta can equal to the output_error.\n",
    "        \"\"\"\n",
    "        output_error = self.output - y\n",
    "        output_delta = output_error\n",
    "\n",
    "        layer1_error = output_delta.dot(self.weights2.T)\n",
    "        layer1_delta = layer1_error * sigmoid_derivative(self.layer1)\n",
    "\n",
    "        \"\"\"\n",
    "        Update the weights and bias for each layer\n",
    "        \"\"\"\n",
    "        self.weights2 -= self.layer1.T.dot(output_delta) * learning_rate\n",
    "        self.bias2 -= np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights1 -= X.T.dot(layer1_delta) * learning_rate\n",
    "        self.bias1 -= np.sum(layer1_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"\n",
    "        The train function is used to train the model for a specified number of epochs. The feedforward and backpropagation functions are called for each epoch. The loss is also computed for each epoch.\n",
    "        :param X: the input data - X\n",
    "        :param y: the target data(class) - y\n",
    "        :param epochs: the number of epochs\n",
    "        :param learning_rate: the learning rate\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.feedforward(X)\n",
    "            self.backpropagation(X, y, learning_rate)\n",
    "            self.cost_history.append(cross_entropy_loss(y, self.output))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function is used to predict the classes for the input data. The feedforward function is called to get the output of the output layer. The class with the highest probability is chosen as the predicted class.\n",
    "        :param X: the input data - X\n",
    "        :return: the predicted classes\n",
    "        \"\"\"\n",
    "        output = self.feedforward(X)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.486320Z",
     "start_time": "2023-11-21T10:39:22.480571Z"
    }
   },
   "id": "cf8277b26ff69fbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1459c333c9520929"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Creating the ANN\n",
    "ann = ANN(X_train.shape[1], 10, y_train.shape[1])\n",
    "\n",
    "# Training the ANN\n",
    "ann.train(X_train, y_train, epochs=1000, learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.605430Z",
     "start_time": "2023-11-21T10:39:22.485916Z"
    }
   },
   "id": "4b100499488a9598"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Model Evaluation\n",
    "## 6.1 Accuracy\n",
    "- I choose use accuracy as my evaluation metric"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "562c8cbb55ed5226"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.65%\n"
     ]
    }
   ],
   "source": [
    "# Predicting classes on the test set\n",
    "y_pred = ann.predict(X_test)\n",
    "\n",
    "# Converting one-hot encoded test labels back to class labels for comparison\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The accuracy function is used to compute the accuracy of the model.\n",
    "    :param y_true: the true labels\n",
    "    :param y_pred: the predicted labels\n",
    "    :return: the accuracy of the model\n",
    "    \"\"\"\n",
    "    return np.sum(y_true == y_pred) / len(y_true) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy(y_test_labels, y_pred):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.605689Z",
     "start_time": "2023-11-21T10:39:22.596634Z"
    }
   },
   "id": "fdfa612859d3b7ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Loss Curve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f8bbbf5a0ff16ee"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1000), ann.cost_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.695263Z",
     "start_time": "2023-11-21T10:39:22.601034Z"
    }
   },
   "id": "84b4a83cdc8ece0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Confusion Matrix\n",
    "- I choose use confusion matrix as my evaluation metric\n",
    "- The confusion matrix is very good, it is very close to the diagonal\n",
    "- So, the model is very good.\n",
    "- Due to the output is multi-class, I choose use softmax as my activation function for output layer, and I choose use cross-entropy as my loss function, it will more fit to train ANN model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30dc7c35b7703214"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  0 13  0  0  0]\n",
      " [ 0  1  0  7  0  0]\n",
      " [ 0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  3]]\n"
     ]
    }
   ],
   "source": [
    "# Creating the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "print(cm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.695463Z",
     "start_time": "2023-11-21T10:39:22.691482Z"
    }
   },
   "id": "37283cb9dce23a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4 Classification Report\n",
    "- I choose use classification report as my evaluation metric"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12059c77a0f3d1da"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        31\n",
      "           1       0.90      1.00      0.95         9\n",
      "           2       1.00      1.00      1.00        13\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.99        74\n",
      "   macro avg       0.98      0.98      0.98        74\n",
      "weighted avg       0.99      0.99      0.99        74\n"
     ]
    }
   ],
   "source": [
    "# Creating the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_labels, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.703508Z",
     "start_time": "2023-11-21T10:39:22.696263Z"
    }
   },
   "id": "e18dfc01c6681422"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.5 MSE\n",
    "MSE (Mean Squared Error) is calculated as follows:\n",
    "$$\n",
    "\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- $\\( n \\)$ is the number of samples.\n",
    "- $\\( y_i \\)$ is the actual value.\n",
    "- $\\( \\hat{y}_i \\)$ is the predicted value.\n",
    "- $\\( (y_i - \\hat{y}_i)^2 \\)$ is the square of the difference between the actual and predicted values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6983cefb753f0213"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.05\n"
     ]
    }
   ],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The mse function is used to compute the mse of the model.\n",
    "    :param y_true: the true labels\n",
    "    :param y_pred: the predicted labels\n",
    "    :return: the mse of the model\n",
    "    \"\"\"\n",
    "    return np.sum((y_true - y_pred) ** 2) / len(y_true)\n",
    "\n",
    "print(f\"MSE: {mse(y_test_labels, y_pred):.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T10:39:22.706768Z",
     "start_time": "2023-11-21T10:39:22.703016Z"
    }
   },
   "id": "386ade51a851dcd5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Conclusion\n",
    "- The accuracy of the model is 97.14%, which is very good.\n",
    "- The loss curve is also very good, it is decreasing.\n",
    "- So, the model is very good.\n",
    "- Due to the output is multi-class, I choose use softmax as my activation function for output layer, and I choose use cross-entropy as my loss function, it will more fit to train ANN model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d544c8f937385e38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
